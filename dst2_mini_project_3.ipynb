{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9150c461",
   "metadata": {},
   "source": [
    "Thad Hoskins\n",
    "\n",
    "Mini-project 3 - Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688f5381",
   "metadata": {},
   "source": [
    "Find some text data of your own choice, it could be labelled tweets, etc. \n",
    "\n",
    "Your dataset should have at least 200 instances, and if there are several columns of text, you can choose to merge the text columns into a single text column. Each text instance should have at least 60 words.\n",
    "\n",
    "For my dataset, I was able to find a labelled collection of tweets used for sentiment analysis.\n",
    "\n",
    "http://help.sentiment140.com/for-students"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebee7316",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB, MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ab3f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db3f654e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-98ae83a959a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tweets.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"polarity\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"query\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"tweets.csv\")\n",
    "tweets.columns = [\"polarity\", \"id\", \"date\", \"query\", \"user\", \"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f76d7c",
   "metadata": {},
   "source": [
    "Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010fbe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.replace({\"polarity\": {4: 1}}, inplace=True)\n",
    "tweets = tweets[(tweets.polarity==0)|(tweets.polarity==1)]\n",
    "\n",
    "tweets.polarity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ed341",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[[\"polarity\", \"text\"]]\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import contractions\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e89c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in text.split(' ')])\n",
    "\n",
    "def expand_contractions(text):\n",
    "    expanded_words = []    \n",
    "    for word in text.split():\n",
    "        expanded_words.append(contractions.fix(word))   \n",
    "\n",
    "    return ' '.join(expanded_words)\n",
    "\n",
    "def remove_others(text):\n",
    "    text = re.sub(r'\\n', \"\", text)\n",
    "    text = re.sub(r'-', \" \", text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r' +', \" \", text)\n",
    "    text = re.sub(r'[\\(\\)\\[\\]\\^\\$\\+\\*\\.\\?\\/!@#%&{}\\'\\\",;:]', \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    stop = set(nltk.corpus.stopwords.words('english'))\n",
    "    cleaned = text.lower()\n",
    "    cleaned = expand_contractions(cleaned)\n",
    "    tokens = word_tokenize(cleaned)\n",
    "    cleaned = ' '.join([w for w in tokens if not w in stop])\n",
    "    cleaned = remove_others(cleaned)\n",
    "    cleaned = lemmatize_text(cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc1d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets[tweets.text.str.len()>=60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca166305",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tweets.head(100).copy()\n",
    "test[\"cleaned_text\"] = test[\"text\"].replace(regex='(@\\w+)|#|&|!',value='')\n",
    "test[\"cleaned_text\"] = test[\"cleaned_text\"].apply(clean_text)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f035f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[tweets.text.str.len()>=60]\n",
    "# tweets = tweets[tweets.text.str.len()>=60].sample(n=2000, random_state=42).copy()\n",
    "tweets.polarity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc3198",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"cleaned_text\"] = tweets[\"text\"].replace(regex='(@\\w+)|#|&|!',value='')\n",
    "tweets[\"cleaned_text\"] = tweets[\"cleaned_text\"].apply(clean_text)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_index = random.randint(0, len(tweets)-1)\n",
    "\n",
    "print(\"News text prior to cleaning.\")\n",
    "tweets.iloc[random_index].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93cd41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"News text after to cleaning.\")\n",
    "tweets.iloc[random_index].cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1853b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.drop([\"text\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f41152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0cdfe",
   "metadata": {},
   "source": [
    "transform the data to a representation suitable for your algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741f7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), stop_words=\"english\", min_df=10, max_features=None)\n",
    "X = tfidf.fit_transform(tweets.cleaned_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58419ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ff686",
   "metadata": {},
   "source": [
    "split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f8fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, tweets.polarity.values,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe35de5",
   "metadata": {},
   "source": [
    "build your model and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa9423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f8bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_train)\n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_pred)}\")\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad058433",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "\n",
    "print(f\"Mean Cross Validation Score: {np.mean(cv_score)}\")\n",
    "print(f\"Cross Validation Score Standard Deviation: {np.std(cv_score)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911b2f0",
   "metadata": {},
   "source": [
    "Tune some parameters of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e10b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tune, X_test_tune, y_train_tune, y_test_tune = train_test_split(tweets.cleaned_text.values,\n",
    "                                                                        tweets.polarity.values,\n",
    "                                                                        test_size=0.3,\n",
    "                                                                        random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08913007",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tune.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a86dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tune.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428cb811",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tune.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9c107",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tune.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7169ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"tfidf\", TfidfVectorizer(stop_words=\"english\")), (\"nb\", MultinomialNB())])\n",
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b586bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"tfidf__min_df\":[.01, .1, .15],\n",
    "              \"tfidf__ngram_range\": [(1,1), (1,2), (2,2)],\n",
    "              \"tfidf__norm\": [None, \"l1\", \"l2\"],\n",
    "              \"nb__alpha\": [.1, .5, 1],\n",
    "              \"nb__fit_prior\": [True, False]}\n",
    "\n",
    "grid_search = GridSearchCV(pipe,\n",
    "                           param_grid, cv=5,\n",
    "                           scoring=\"accuracy\",\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(X_train_tune, y_train_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7eccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_est = grid_search.best_estimator_\n",
    "best_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8091692",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d02174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_est, \"assignment_5_part4_best.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd3c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Score on training set: {best_est.score(X_train_tune, y_train_tune)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe91af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Score on test set: {best_est.score(X_test_tune, y_test_tune)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a647c6f",
   "metadata": {},
   "source": [
    "The main problem was finding the dataset! Using the above guidance, I looked for a tweet dataset that was labelled. I found one here (referenced above):<br>\n",
    "http://help.sentiment140.com/for-students\n",
    "\n",
    "The dataset included a number of fields. However, I only needed the \"polarity\" and the text of the tweets. Polarity is my label or target. At the start, the values were:\n",
    "<ul>\n",
    "    <li>0 = negative</li>\n",
    "    <li>2 = neutral</li>\n",
    "    <li>4 = positive</li>\n",
    "</ul>\n",
    "\n",
    "I am targeting wanting to know if a tweet is positive or negative, so I dropped neutral and recoded positive to 1.\n",
    "\n",
    "To meet the requirement, I limited the dataset to tweets that were more than 60 characters. I decided to do this before transforming the data.\n",
    "\n",
    "As for my text, I used code I wrote for a project in DS Tools 1 to \"clean\" the text, i.e., I did not use the cleaning provided in teh lecture. For my cleaning, I performed the following:\n",
    "<ul>\n",
    "    <li>Removed @user from every tweet</li>\n",
    "    <li>Made all letters lower case</li>\n",
    "    <li>expanded contractions</li>\n",
    "    <li>Removed stop words</li>\n",
    "    <li>Remove characters</li>\n",
    "    <li>Lemmatized the words</li>\n",
    "</ul>\n",
    "\n",
    "Following the procedure from #3 from this assignment, I then vectorized the tweet text. Next, the data was split into training and test data (feature and label for each), then fitted the model using MultinomialNB. I then cross validated the results, outputting scores at every steps.\n",
    "\n",
    "With the ball rolling downhill, I then went back to the non-vectorized dataset.\n",
    "\n",
    "With a pipeline constructed that will both vectorize and fit the model, I put those pieces in place, then ran the pipeline, both transforming the data and fitting the model.\n",
    "\n",
    "My chosen hyper-parameters check for minimum usage of terms, ngram range (checking for groupings of words), and row output normalization (norm) in the tranformation, as well as the smoothing factor (alpha) and a probability learning flag (learn_prior) for the Naive Bayes model.\n",
    "\n",
    "Given the size of the dataset, the cleaning takes a long time, as does the tuning. Times like this make me wish to build a Data Science computer.\n",
    "\n",
    "In the end, the cross validated gridsearch yielded worse scores. Given the length of run time for a homework assignment I can then only guess to the reason for such a result. My two competing theories are overfitting and limiting hyper-parameters for tuning.\n",
    "\n",
    "The first is overfitting. The initial  model overfit with an accuracy score of nearly 75%. The tuning and cross validation may prove this out with a much lower accuracy  of 64.8% for the training and 64% for the test. That would indicate the classification power of the model is stronger than a coin toss, but not remarkable so.\n",
    "\n",
    "The second reason could be tested more thoroughly with a better system than my laptop and more time and that is that my hyper-parameters were restrictive to creating a worse model. My original parameters were:\n",
    "{'alpha': 1.0, 'class_prior': None, 'fit_prior': True}<br>\n",
    "{'analyzer': 'word',\n",
    " 'binary': False,\n",
    " 'decode_error': 'strict',\n",
    " 'dtype': numpy.float64,\n",
    " 'encoding': 'utf-8',\n",
    " 'input': 'content',\n",
    " 'lowercase': True,\n",
    " 'max_df': 1.0,\n",
    " 'max_features': None,\n",
    " 'min_df': 10,\n",
    " 'ngram_range': (1, 2),\n",
    " 'norm': 'l2',\n",
    " 'preprocessor': None,\n",
    " 'smooth_idf': True,\n",
    " 'stop_words': 'english',\n",
    " 'strip_accents': None,\n",
    " 'sublinear_tf': False,\n",
    " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    " 'tokenizer': None,\n",
    " 'use_idf': True,\n",
    " 'vocabulary': None}\n",
    " \n",
    "The tuned parameters are:\n",
    " {'nb__alpha': 0.1,\n",
    " 'nb__fit_prior': True,\n",
    " 'tfidf__min_df': 0.01,\n",
    " 'tfidf__ngram_range': (1, 1),\n",
    " 'tfidf__norm': 'l2'}\n",
    " \n",
    "Noteable differences are the alpha (for the model) and min_df (data transformation). \n",
    "\n",
    "The tuned min_df is 10% which is quite higher than the 10 overall records of the original model.\n",
    " \n",
    "I had more tuning features but runtime was a limitation.\n",
    " \n",
    "Solutions in the future would be to limit the dataset significantly. The original dataset was 1.6 million, reduced to 900k with some filtering. I then further randomly sampled the data to 500k (further leaning toward cross validation doing its job). Even so, the data cleaning, transformation, fitting, and tuning was very time consuming. I can reduce those now by saving the dataset so that I do not have to do much of that. I can also save the model, change some parameters, and compare that to this one without having to fit again.\n",
    " \n",
    "I would recommend tweaks to the model to increase the accuracy before taking this to production. The method is sound, but needs further tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
